{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Mini-project\n",
    "### Analysis of ResNet-style Architectures on CIFAR-10 Using lesser than Five-million Parameters\n",
    "\n",
    "<br>\n",
    "Anudeep Tubati, Ashwin Guptha, Aditya Shyamsundar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuralFlux/dl-mini-project/blob/main/resnet_method1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xjb0WKDheuDR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import copy\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n19IX-9ylCk",
    "outputId": "d185af02-ec7d-405c-cc8e-b028fa125059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:29<00:00, 5861822.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n"
     ]
    }
   ],
   "source": [
    "sample_data = CIFAR10('./', download=True)\n",
    "\n",
    "\n",
    "means = sample_data.data.mean(axis=(0, 1, 2)) / 255\n",
    "stds = sample_data.data.std(axis=(0, 1, 2)) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pMTV0bUWx0cM"
   },
   "outputs": [],
   "source": [
    "# Creating the transforms to augment the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomApply(  \n",
    "        [\n",
    "            transforms.ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5),\n",
    "                                   saturation=(0.5, 1.5), hue=(-0.5, 0.5)),\n",
    "        ],\n",
    "        p=0.5\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "  ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaSXtPYT09Vh",
    "outputId": "d23a66ab-a2e2-4d7d-f7f1-7ab93b9672d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = CIFAR10('./', download=True, transform=transform_train)\n",
    "test_data = CIFAR10('./', train=False, download=True,\n",
    "                             transform=transform_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dzF3VRpyzEG4"
   },
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "num_train_samples = int(len(train_data) * TRAIN_RATIO)\n",
    "num_valid_samples = len(train_data) - num_train_samples\n",
    "split = [num_train_samples, num_valid_samples]\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, lengths=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Fgng2RZU1pyG"
   },
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transforms = transform_train\n",
    "     \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n",
    "valid_iterator = data.DataLoader(valid_data, BATCH_SIZE)\n",
    "test_iterator = data.DataLoader(test_data, BATCH_SIZE)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V_Xl1ZH115T0"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_iterator, \"val\": valid_iterator}\n",
    "dataset_sizes = {\"train\": len(train_data.indices),\n",
    "                 'val': len(valid_data.indices)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "TpmyPAoNl-Ls"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ABlWhIT7mEMb"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        #og kernel_size=7,stride=2 change to 3, stride = 1\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=1, stride=1, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #og stride=2 change to stride = 1 for MaXPoolLayer and all make_layers\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Qghg6YG3mp2j"
   },
   "outputs": [],
   "source": [
    "def _resnet(block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ibdoufO_174J"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':  # take scheduler step on train acc\n",
    "                scheduler.step(epoch_acc)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "u1XeCF57nCat",
    "outputId": "54c420ea-23ec-4517-9f75-e34b6efdd4d7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[100, 150])\\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\n                                                       factor=0.1,\\n                                                       patience=5,\\n                                                       verbose=True)\\n                                                    \\n#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = _resnet(BasicBlock, [2, 2, 2, 2]).to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       patience=10,\n",
    "                                                       verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7vfNVL3ofz1",
    "outputId": "64ab3cd3-71c3-4caa-dac7-125e74c5515b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "----------\n",
      "train Loss: 0.0067 Acc: 0.3892\n",
      "val Loss: 0.0063 Acc: 0.4166\n",
      "\n",
      "Epoch 2/50\n",
      "----------\n",
      "train Loss: 0.0050 Acc: 0.5414\n",
      "val Loss: 0.0059 Acc: 0.4966\n",
      "\n",
      "Epoch 3/50\n",
      "----------\n",
      "train Loss: 0.0042 Acc: 0.6155\n",
      "val Loss: 0.0047 Acc: 0.6060\n",
      "\n",
      "Epoch 4/50\n",
      "----------\n",
      "train Loss: 0.0036 Acc: 0.6713\n",
      "val Loss: 0.0043 Acc: 0.6320\n",
      "\n",
      "Epoch 5/50\n",
      "----------\n",
      "train Loss: 0.0032 Acc: 0.7146\n",
      "val Loss: 0.0052 Acc: 0.5998\n",
      "\n",
      "Epoch 6/50\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.7409\n",
      "val Loss: 0.0038 Acc: 0.6758\n",
      "\n",
      "Epoch 7/50\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.7660\n",
      "val Loss: 0.0035 Acc: 0.6960\n",
      "\n",
      "Epoch 8/50\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.7818\n",
      "val Loss: 0.0042 Acc: 0.6502\n",
      "\n",
      "Epoch 9/50\n",
      "----------\n",
      "train Loss: 0.0024 Acc: 0.7930\n",
      "val Loss: 0.0036 Acc: 0.6880\n",
      "\n",
      "Epoch 10/50\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.7980\n",
      "val Loss: 0.0032 Acc: 0.7266\n",
      "\n",
      "Epoch 11/50\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.8020\n",
      "val Loss: 0.0033 Acc: 0.7160\n",
      "\n",
      "Epoch 12/50\n",
      "----------\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-02.\n",
      "train Loss: 0.0022 Acc: 0.8102\n",
      "val Loss: 0.0042 Acc: 0.6658\n",
      "\n",
      "Epoch 13/50\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.8769\n",
      "val Loss: 0.0016 Acc: 0.8596\n",
      "\n",
      "Epoch 14/50\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.8976\n",
      "val Loss: 0.0016 Acc: 0.8650\n",
      "\n",
      "Epoch 15/50\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 0.9092\n",
      "val Loss: 0.0015 Acc: 0.8662\n",
      "\n",
      "Epoch 16/50\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 0.9159\n",
      "val Loss: 0.0016 Acc: 0.8662\n",
      "\n",
      "Epoch 17/50\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9206\n",
      "val Loss: 0.0015 Acc: 0.8722\n",
      "\n",
      "Epoch 18/50\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9278\n",
      "val Loss: 0.0015 Acc: 0.8710\n",
      "\n",
      "Epoch 19/50\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9318\n",
      "val Loss: 0.0015 Acc: 0.8766\n",
      "\n",
      "Epoch 20/50\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9370\n",
      "val Loss: 0.0015 Acc: 0.8720\n",
      "\n",
      "Epoch 21/50\n",
      "----------\n",
      "train Loss: 0.0007 Acc: 0.9412\n",
      "val Loss: 0.0017 Acc: 0.8648\n",
      "\n",
      "Epoch 22/50\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9452\n",
      "val Loss: 0.0016 Acc: 0.8660\n",
      "\n",
      "Epoch 23/50\n",
      "----------\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-03.\n",
      "train Loss: 0.0006 Acc: 0.9478\n",
      "val Loss: 0.0017 Acc: 0.8658\n",
      "\n",
      "Epoch 24/50\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9658\n",
      "val Loss: 0.0014 Acc: 0.8894\n",
      "\n",
      "Epoch 25/50\n",
      "----------\n",
      "train Loss: 0.0004 Acc: 0.9718\n",
      "val Loss: 0.0013 Acc: 0.8864\n",
      "\n",
      "Epoch 26/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9744\n",
      "val Loss: 0.0013 Acc: 0.8932\n",
      "\n",
      "Epoch 27/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9775\n",
      "val Loss: 0.0014 Acc: 0.8906\n",
      "\n",
      "Epoch 28/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9768\n",
      "val Loss: 0.0014 Acc: 0.8922\n",
      "\n",
      "Epoch 29/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9792\n",
      "val Loss: 0.0013 Acc: 0.8902\n",
      "\n",
      "Epoch 30/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9799\n",
      "val Loss: 0.0014 Acc: 0.8898\n",
      "\n",
      "Epoch 31/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9810\n",
      "val Loss: 0.0014 Acc: 0.8910\n",
      "\n",
      "Epoch 32/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9812\n",
      "val Loss: 0.0014 Acc: 0.8942\n",
      "\n",
      "Epoch 33/50\n",
      "----------\n",
      "train Loss: 0.0003 Acc: 0.9812\n",
      "val Loss: 0.0014 Acc: 0.8922\n",
      "\n",
      "Epoch 34/50\n",
      "----------\n",
      "Epoch 00034: reducing learning rate of group 0 to 1.0000e-04.\n",
      "train Loss: 0.0002 Acc: 0.9833\n",
      "val Loss: 0.0014 Acc: 0.8892\n",
      "\n",
      "Epoch 35/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9858\n",
      "val Loss: 0.0014 Acc: 0.8894\n",
      "\n",
      "Epoch 36/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9859\n",
      "val Loss: 0.0014 Acc: 0.8972\n",
      "\n",
      "Epoch 37/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9854\n",
      "val Loss: 0.0014 Acc: 0.8858\n",
      "\n",
      "Epoch 38/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9862\n",
      "val Loss: 0.0014 Acc: 0.8930\n",
      "\n",
      "Epoch 39/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9849\n",
      "val Loss: 0.0013 Acc: 0.8960\n",
      "\n",
      "Epoch 40/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9869\n",
      "val Loss: 0.0014 Acc: 0.8918\n",
      "\n",
      "Epoch 41/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9862\n",
      "val Loss: 0.0014 Acc: 0.8892\n",
      "\n",
      "Epoch 42/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9856\n",
      "val Loss: 0.0013 Acc: 0.8974\n",
      "\n",
      "Epoch 43/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9856\n",
      "val Loss: 0.0014 Acc: 0.8888\n",
      "\n",
      "Epoch 44/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9857\n",
      "val Loss: 0.0014 Acc: 0.8912\n",
      "\n",
      "Epoch 45/50\n",
      "----------\n",
      "Epoch 00045: reducing learning rate of group 0 to 1.0000e-05.\n",
      "train Loss: 0.0002 Acc: 0.9859\n",
      "val Loss: 0.0014 Acc: 0.8954\n",
      "\n",
      "Epoch 46/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9862\n",
      "val Loss: 0.0014 Acc: 0.8904\n",
      "\n",
      "Epoch 47/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9868\n",
      "val Loss: 0.0014 Acc: 0.8978\n",
      "\n",
      "Epoch 48/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9868\n",
      "val Loss: 0.0014 Acc: 0.8908\n",
      "\n",
      "Epoch 49/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9868\n",
      "val Loss: 0.0014 Acc: 0.8902\n",
      "\n",
      "Epoch 50/50\n",
      "----------\n",
      "train Loss: 0.0002 Acc: 0.9876\n",
      "val Loss: 0.0014 Acc: 0.8914\n",
      "\n",
      "Training complete in 64m 25s\n",
      "Best val Acc: 0.897800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggeMj-HafnE0",
    "outputId": "0d0e9bc4-7e48-4950-8084-dd6ae655122c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 89.15 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_iterator:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the test images: {} %'.format(100 * correct / total))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgousbDYgYAE",
    "outputId": "2642c5a1-3f18-47f8-c331-c76298846ca9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8066344"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP139QrHHzYYpCQoVzKOos6",
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

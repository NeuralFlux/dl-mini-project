{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Mini-project\n",
    "### Analysis of ResNet-style Architectures on CIFAR-10 Using lesser than Five-million Parameters\n",
    "\n",
    "<br>\n",
    "Anudeep Tubati, Ashwin Guptha, Aditya Shyamsundar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuralFlux/dl-mini-project/blob/main/resnet_method3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjb0WKDheuDR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import copy\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2n19IX-9ylCk",
    "outputId": "16fe8dac-dc4f-47c1-d776-08321b7f4f8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:03<00:00, 44624347.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./cifar-10-python.tar.gz to ./\n"
     ]
    }
   ],
   "source": [
    "sample_data = CIFAR10('./', download=True)\n",
    "\n",
    "\n",
    "means = sample_data.data.mean(axis=(0, 1, 2)) / 255\n",
    "stds = sample_data.data.std(axis=(0, 1, 2)) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMTV0bUWx0cM"
   },
   "outputs": [],
   "source": [
    "# Creating the transforms to augment the dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation((-90,90)),\n",
    "    transforms.RandomApply(  \n",
    "        [\n",
    "            transforms.ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5),\n",
    "                                   saturation=(0.5, 1.5), hue=(-0.5, 0.5)),\n",
    "        ],\n",
    "        p=0.5\n",
    "    ),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "  ])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AaSXtPYT09Vh",
    "outputId": "1f761d02-780e-4cb3-f984-e0d21dfc28b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = CIFAR10('./', download=True, transform=transform_train)\n",
    "test_data = CIFAR10('./', train=False, download=True,\n",
    "                             transform=transform_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzF3VRpyzEG4"
   },
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.9\n",
    "num_train_samples = int(len(train_data) * TRAIN_RATIO)\n",
    "num_valid_samples = len(train_data) - num_train_samples\n",
    "split = [num_train_samples, num_valid_samples]\n",
    "\n",
    "train_data, valid_data = data.random_split(train_data, lengths=split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fgng2RZU1pyG"
   },
   "outputs": [],
   "source": [
    "valid_data = copy.deepcopy(valid_data)\n",
    "valid_data.dataset.transforms = transform_train\n",
    "     \n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_iterator = data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n",
    "valid_iterator = data.DataLoader(valid_data, BATCH_SIZE)\n",
    "test_iterator = data.DataLoader(test_data, BATCH_SIZE)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_Xl1ZH115T0"
   },
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": train_iterator, \"val\": valid_iterator}\n",
    "dataset_sizes = {\"train\": len(train_data.indices),\n",
    "                 'val': len(valid_data.indices)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modified ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpmyPAoNl-Ls"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABlWhIT7mEMb"
   },
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        #og kernel_size=7,stride=2 change to 3, stride = 1\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=1, stride=1, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        #og stride=2 change to stride = 1 for MaXPoolLayer and all make_layers\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=1, stride=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=1)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        #self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        #x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qghg6YG3mp2j"
   },
   "outputs": [],
   "source": [
    "def _resnet(block, layers, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibdoufO_174J"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.to(DEVICE)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item()\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            if phase == 'train':  # take scheduler step on train acc\n",
    "                scheduler.step(epoch_acc)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "u1XeCF57nCat",
    "outputId": "8fa66ef5-bf22-4252-e592-5ddf1a293537"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,milestones=[100, 150])\\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\n                                                       factor=0.1,\\n                                                       patience=5,\\n                                                       verbose=True)\\n                                                    \\n#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = _resnet(BasicBlock, [1, 4, 3, 0]).to(DEVICE)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9) \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       factor=0.1,\n",
    "                                                       patience=10,\n",
    "                                                       verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r7vfNVL3ofz1",
    "outputId": "169cf510-b9b2-497f-b523-43b1617ba172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n",
      "train Loss: 0.0096 Acc: 0.1863\n",
      "val Loss: 0.0081 Acc: 0.2520\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "train Loss: 0.0071 Acc: 0.3174\n",
      "val Loss: 0.0087 Acc: 0.2428\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "train Loss: 0.0057 Acc: 0.4719\n",
      "val Loss: 0.0063 Acc: 0.4572\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "train Loss: 0.0045 Acc: 0.5885\n",
      "val Loss: 0.0053 Acc: 0.5238\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "train Loss: 0.0039 Acc: 0.6476\n",
      "val Loss: 0.0047 Acc: 0.6060\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "train Loss: 0.0033 Acc: 0.7020\n",
      "val Loss: 0.0061 Acc: 0.5366\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "train Loss: 0.0029 Acc: 0.7427\n",
      "val Loss: 0.0035 Acc: 0.6960\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "train Loss: 0.0026 Acc: 0.7681\n",
      "val Loss: 0.0045 Acc: 0.6420\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "train Loss: 0.0025 Acc: 0.7841\n",
      "val Loss: 0.0042 Acc: 0.6780\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "train Loss: 0.0023 Acc: 0.7974\n",
      "val Loss: 0.0035 Acc: 0.7268\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "train Loss: 0.0022 Acc: 0.8051\n",
      "val Loss: 0.0051 Acc: 0.5906\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-02.\n",
      "train Loss: 0.0021 Acc: 0.8137\n",
      "val Loss: 0.0036 Acc: 0.7104\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "train Loss: 0.0014 Acc: 0.8793\n",
      "val Loss: 0.0015 Acc: 0.8688\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "train Loss: 0.0012 Acc: 0.8995\n",
      "val Loss: 0.0015 Acc: 0.8774\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "train Loss: 0.0011 Acc: 0.9050\n",
      "val Loss: 0.0014 Acc: 0.8766\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 0.9124\n",
      "val Loss: 0.0014 Acc: 0.8812\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "train Loss: 0.0010 Acc: 0.9182\n",
      "val Loss: 0.0014 Acc: 0.8792\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9227\n",
      "val Loss: 0.0014 Acc: 0.8796\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "train Loss: 0.0009 Acc: 0.9276\n",
      "val Loss: 0.0013 Acc: 0.8896\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9302\n",
      "val Loss: 0.0013 Acc: 0.8912\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9354\n",
      "val Loss: 0.0013 Acc: 0.8936\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "train Loss: 0.0008 Acc: 0.9375\n",
      "val Loss: 0.0014 Acc: 0.8834\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-03.\n",
      "train Loss: 0.0007 Acc: 0.9403\n",
      "val Loss: 0.0013 Acc: 0.8930\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "train Loss: 0.0006 Acc: 0.9554\n",
      "val Loss: 0.0011 Acc: 0.9116\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9615\n",
      "val Loss: 0.0011 Acc: 0.9026\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9629\n",
      "val Loss: 0.0011 Acc: 0.9112\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9636\n",
      "val Loss: 0.0011 Acc: 0.9062\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9661\n",
      "val Loss: 0.0011 Acc: 0.9100\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9662\n",
      "val Loss: 0.0011 Acc: 0.9068\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "train Loss: 0.0005 Acc: 0.9674\n",
      "val Loss: 0.0011 Acc: 0.9088\n",
      "\n",
      "Training complete in 99m 54s\n",
      "Best val Acc: 0.911600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=256, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, criterion, optimizer, scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggeMj-HafnE0",
    "outputId": "a815b661-0513-410e-98cd-1c600ee342e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test images: 91.08 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_iterator:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy on the test images: {} %'.format(100 * correct / total))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UgousbDYgYAE",
    "outputId": "c98137f5-b3d6-466d-fafe-f2d1560871d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4728104"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

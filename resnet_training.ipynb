{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHPXq2DWEN-J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.datasets import cifar\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "\n",
        "sample_data = cifar.CIFAR10('./', download=True)\n",
        "\n",
        "means = sample_data.data.mean(axis=(0, 1, 2)) / 255\n",
        "stds = sample_data.data.std(axis=(0, 1, 2)) / 255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jitter_transform = transforms.RandomApply([\n",
        "                                            transforms.ColorJitter(\n",
        "                                            brightness=(0.5, 1.5),\n",
        "                                            contrast=(0.5, 1.5),\n",
        "                                            saturation=(0.5, 1.5),\n",
        "                                            hue=(-0.5, 0.5))],\n",
        "                                           p=0.5)\n",
        "\n",
        "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.RandomResizedCrop((32, 32),\n",
        "                                                                    (0.5, 1.0)),\n",
        "                                       jitter_transform,\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(means, stds)])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                      transforms.Normalize(means, stds)])\n",
        "\n",
        "train_data = cifar.CIFAR10('./', download=True, transform=train_transforms)\n",
        "test_data = cifar.CIFAR10('./', train=False, download=True,\n",
        "                             transform=test_transforms)"
      ],
      "metadata": {
        "id": "clgfi5f63v7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO = 0.9\n",
        "num_train_samples = int(len(train_data) * TRAIN_RATIO)\n",
        "num_valid_samples = len(train_data) - num_train_samples\n",
        "split = [num_train_samples, num_valid_samples]\n",
        "\n",
        "train_data, valid_data = data.random_split(train_data, lengths=split)"
      ],
      "metadata": {
        "id": "i6WF0sLHEpGr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transforms = test_transforms"
      ],
      "metadata": {
        "id": "27LJJCmnFA8b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_iterator = data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n",
        "valid_iterator = data.DataLoader(valid_data, BATCH_SIZE)\n",
        "test_iterator = data.DataLoader(test_data, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "z7y5OgnIWGl7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\"train\": train_iterator, \"val\": valid_iterator}\n",
        "dataset_sizes = {\"train\": len(train_data.indices),\n",
        "                 'val': len(valid_data.indices)}"
      ],
      "metadata": {
        "id": "TAfhGR-mYGsc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':  # take scheduler step on train acc\n",
        "                scheduler.step(epoch_acc)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "yH5ZK5_pQeZS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 200\n",
        "LR = 0.1\n",
        "LR_DECAY = 0.1\n",
        "\n",
        "model = resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 10)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# same parameters as the ResNet paper\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.99,\n",
        "                            weight_decay=1e-4)\n",
        "\n",
        "# patience is not known from the paper\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       factor=LR_DECAY,\n",
        "                                                       patience=NUM_EPOCHS / 10,\n",
        "                                                       verbose=True)\n",
        "\n",
        "model.to(DEVICE)\n",
        "criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "LZh2hSjOL_8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0cUIZOKXRro",
        "outputId": "b9f7517a-3c86-4de3-fa98-51f249389931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "train Loss: 0.0102 Acc: 0.1635\n",
            "val Loss: 0.0086 Acc: 0.2032\n",
            "\n",
            "Epoch 1/199\n",
            "----------\n",
            "train Loss: 0.0079 Acc: 0.2370\n",
            "val Loss: 0.0080 Acc: 0.2288\n",
            "\n",
            "Epoch 2/199\n",
            "----------\n",
            "train Loss: 0.0076 Acc: 0.2760\n",
            "val Loss: 0.0078 Acc: 0.2722\n",
            "\n",
            "Epoch 3/199\n",
            "----------\n",
            "train Loss: 0.0075 Acc: 0.2827\n",
            "val Loss: 0.0076 Acc: 0.2950\n",
            "\n",
            "Epoch 4/199\n",
            "----------\n",
            "train Loss: 0.0084 Acc: 0.2024\n",
            "val Loss: 0.0090 Acc: 0.1508\n",
            "\n",
            "Epoch 5/199\n",
            "----------\n",
            "train Loss: 0.0082 Acc: 0.1780\n",
            "val Loss: 0.0083 Acc: 0.1940\n",
            "\n",
            "Epoch 6/199\n",
            "----------\n",
            "train Loss: 0.0080 Acc: 0.2067\n",
            "val Loss: 0.0083 Acc: 0.2022\n",
            "\n",
            "Epoch 7/199\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 0.2306\n",
            "val Loss: 0.0079 Acc: 0.2454\n",
            "\n",
            "Epoch 8/199\n",
            "----------\n",
            "train Loss: 0.0075 Acc: 0.2517\n",
            "val Loss: 0.0079 Acc: 0.2308\n",
            "\n",
            "Epoch 9/199\n",
            "----------\n",
            "train Loss: 0.0076 Acc: 0.2531\n",
            "val Loss: 0.0076 Acc: 0.2636\n",
            "\n",
            "Epoch 10/199\n",
            "----------\n",
            "train Loss: 0.0074 Acc: 0.2663\n",
            "val Loss: 0.0076 Acc: 0.2528\n",
            "\n",
            "Epoch 11/199\n",
            "----------\n",
            "train Loss: 0.0072 Acc: 0.2890\n",
            "val Loss: 0.0073 Acc: 0.2844\n",
            "\n",
            "Epoch 12/199\n",
            "----------\n",
            "train Loss: 0.0071 Acc: 0.2983\n",
            "val Loss: 0.0073 Acc: 0.3080\n",
            "\n",
            "Epoch 13/199\n",
            "----------\n",
            "train Loss: 0.0069 Acc: 0.3313\n",
            "val Loss: 0.0072 Acc: 0.3202\n",
            "\n",
            "Epoch 14/199\n",
            "----------\n",
            "train Loss: 0.0066 Acc: 0.3641\n",
            "val Loss: 0.0068 Acc: 0.3594\n",
            "\n",
            "Epoch 15/199\n",
            "----------\n",
            "train Loss: 0.0066 Acc: 0.3758\n",
            "val Loss: 0.0067 Acc: 0.3774\n",
            "\n",
            "Epoch 16/199\n",
            "----------\n",
            "train Loss: 0.0063 Acc: 0.4135\n",
            "val Loss: 0.0064 Acc: 0.4224\n",
            "\n",
            "Epoch 17/199\n",
            "----------\n",
            "train Loss: 0.0062 Acc: 0.4300\n",
            "val Loss: 0.0064 Acc: 0.4178\n",
            "\n",
            "Epoch 18/199\n",
            "----------\n",
            "train Loss: 0.0060 Acc: 0.4567\n",
            "val Loss: 0.0061 Acc: 0.4486\n",
            "\n",
            "Epoch 19/199\n",
            "----------\n",
            "train Loss: 0.0059 Acc: 0.4681\n",
            "val Loss: 0.0065 Acc: 0.4416\n",
            "\n",
            "Epoch 20/199\n",
            "----------\n",
            "train Loss: 0.0057 Acc: 0.4807\n",
            "val Loss: 0.0057 Acc: 0.5052\n",
            "\n",
            "Epoch 21/199\n",
            "----------\n",
            "Epoch 00022: reducing learning rate of group 0 to 1.0000e-02.\n",
            "train Loss: 0.0056 Acc: 0.4939\n",
            "val Loss: 0.0060 Acc: 0.4584\n",
            "\n",
            "Epoch 22/199\n",
            "----------\n",
            "train Loss: 0.0050 Acc: 0.5482\n",
            "val Loss: 0.0048 Acc: 0.5778\n",
            "\n",
            "Epoch 23/199\n",
            "----------\n",
            "train Loss: 0.0047 Acc: 0.5762\n",
            "val Loss: 0.0046 Acc: 0.5898\n",
            "\n",
            "Epoch 24/199\n",
            "----------\n",
            "train Loss: 0.0045 Acc: 0.5889\n",
            "val Loss: 0.0046 Acc: 0.5946\n",
            "\n",
            "Epoch 25/199\n",
            "----------\n",
            "train Loss: 0.0044 Acc: 0.5960\n",
            "val Loss: 0.0045 Acc: 0.6088\n",
            "\n",
            "Epoch 26/199\n",
            "----------\n",
            "train Loss: 0.0043 Acc: 0.6090\n",
            "val Loss: 0.0043 Acc: 0.6206\n",
            "\n",
            "Epoch 27/199\n",
            "----------\n",
            "train Loss: 0.0042 Acc: 0.6212\n",
            "val Loss: 0.0042 Acc: 0.6320\n",
            "\n",
            "Epoch 28/199\n",
            "----------\n",
            "train Loss: 0.0041 Acc: 0.6271\n",
            "val Loss: 0.0043 Acc: 0.6312\n",
            "\n",
            "Epoch 29/199\n",
            "----------\n",
            "train Loss: 0.0041 Acc: 0.6324\n",
            "val Loss: 0.0041 Acc: 0.6464\n",
            "\n",
            "Epoch 30/199\n",
            "----------\n",
            "train Loss: 0.0040 Acc: 0.6388\n",
            "val Loss: 0.0041 Acc: 0.6428\n",
            "\n",
            "Epoch 31/199\n",
            "----------\n",
            "train Loss: 0.0039 Acc: 0.6471\n",
            "val Loss: 0.0041 Acc: 0.6444\n",
            "\n",
            "Epoch 32/199\n",
            "----------\n",
            "train Loss: 0.0039 Acc: 0.6540\n",
            "val Loss: 0.0040 Acc: 0.6562\n",
            "\n",
            "Epoch 33/199\n",
            "----------\n",
            "train Loss: 0.0038 Acc: 0.6638\n",
            "val Loss: 0.0039 Acc: 0.6662\n",
            "\n",
            "Epoch 34/199\n",
            "----------\n",
            "train Loss: 0.0037 Acc: 0.6700\n",
            "val Loss: 0.0039 Acc: 0.6660\n",
            "\n",
            "Epoch 35/199\n",
            "----------\n",
            "train Loss: 0.0037 Acc: 0.6741\n",
            "val Loss: 0.0038 Acc: 0.6750\n",
            "\n",
            "Epoch 36/199\n",
            "----------\n",
            "train Loss: 0.0036 Acc: 0.6770\n",
            "val Loss: 0.0037 Acc: 0.6862\n",
            "\n",
            "Epoch 37/199\n",
            "----------\n",
            "train Loss: 0.0035 Acc: 0.6836\n",
            "val Loss: 0.0037 Acc: 0.6770\n",
            "\n",
            "Epoch 38/199\n",
            "----------\n",
            "train Loss: 0.0034 Acc: 0.6942\n",
            "val Loss: 0.0037 Acc: 0.6824\n",
            "\n",
            "Epoch 39/199\n",
            "----------\n",
            "train Loss: 0.0033 Acc: 0.7000\n",
            "val Loss: 0.0037 Acc: 0.6770\n",
            "\n",
            "Epoch 40/199\n",
            "----------\n",
            "train Loss: 0.0034 Acc: 0.7020\n",
            "val Loss: 0.0036 Acc: 0.6886\n",
            "\n",
            "Epoch 41/199\n",
            "----------\n",
            "train Loss: 0.0033 Acc: 0.7088\n",
            "val Loss: 0.0036 Acc: 0.6890\n",
            "\n",
            "Epoch 42/199\n",
            "----------\n",
            "Epoch 00043: reducing learning rate of group 0 to 1.0000e-03.\n",
            "train Loss: 0.0033 Acc: 0.7090\n",
            "val Loss: 0.0035 Acc: 0.7042\n",
            "\n",
            "Epoch 43/199\n",
            "----------\n",
            "train Loss: 0.0030 Acc: 0.7311\n",
            "val Loss: 0.0033 Acc: 0.7118\n",
            "\n",
            "Epoch 44/199\n",
            "----------\n",
            "train Loss: 0.0029 Acc: 0.7442\n",
            "val Loss: 0.0032 Acc: 0.7272\n",
            "\n",
            "Epoch 45/199\n",
            "----------\n",
            "train Loss: 0.0029 Acc: 0.7438\n",
            "val Loss: 0.0032 Acc: 0.7260\n",
            "\n",
            "Epoch 46/199\n",
            "----------\n",
            "train Loss: 0.0029 Acc: 0.7454\n",
            "val Loss: 0.0031 Acc: 0.7356\n",
            "\n",
            "Epoch 47/199\n",
            "----------\n",
            "train Loss: 0.0028 Acc: 0.7507\n",
            "val Loss: 0.0031 Acc: 0.7358\n",
            "\n",
            "Epoch 48/199\n",
            "----------\n",
            "train Loss: 0.0028 Acc: 0.7505\n",
            "val Loss: 0.0031 Acc: 0.7436\n",
            "\n",
            "Epoch 49/199\n",
            "----------\n",
            "train Loss: 0.0028 Acc: 0.7564\n",
            "val Loss: 0.0031 Acc: 0.7334\n",
            "\n",
            "Epoch 50/199\n",
            "----------\n",
            "train Loss: 0.0028 Acc: 0.7544\n",
            "val Loss: 0.0031 Acc: 0.7348\n",
            "\n",
            "Epoch 51/199\n",
            "----------\n",
            "train Loss: 0.0028 Acc: 0.7530\n",
            "val Loss: 0.0030 Acc: 0.7320\n",
            "\n",
            "Epoch 52/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7584\n",
            "val Loss: 0.0031 Acc: 0.7392\n",
            "\n",
            "Epoch 53/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7555\n",
            "val Loss: 0.0030 Acc: 0.7418\n",
            "\n",
            "Epoch 54/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7628\n",
            "val Loss: 0.0031 Acc: 0.7342\n",
            "\n",
            "Epoch 55/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7608\n",
            "val Loss: 0.0030 Acc: 0.7410\n",
            "\n",
            "Epoch 56/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7634\n",
            "val Loss: 0.0030 Acc: 0.7470\n",
            "\n",
            "Epoch 57/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7639\n",
            "val Loss: 0.0029 Acc: 0.7510\n",
            "\n",
            "Epoch 58/199\n",
            "----------\n",
            "train Loss: 0.0026 Acc: 0.7655\n",
            "val Loss: 0.0029 Acc: 0.7386\n",
            "\n",
            "Epoch 59/199\n",
            "----------\n",
            "train Loss: 0.0026 Acc: 0.7676\n",
            "val Loss: 0.0029 Acc: 0.7440\n",
            "\n",
            "Epoch 60/199\n",
            "----------\n",
            "train Loss: 0.0026 Acc: 0.7701\n",
            "val Loss: 0.0030 Acc: 0.7414\n",
            "\n",
            "Epoch 61/199\n",
            "----------\n",
            "train Loss: 0.0026 Acc: 0.7693\n",
            "val Loss: 0.0030 Acc: 0.7354\n",
            "\n",
            "Epoch 62/199\n",
            "----------\n",
            "train Loss: 0.0026 Acc: 0.7734\n",
            "val Loss: 0.0029 Acc: 0.7470\n",
            "\n",
            "Epoch 63/199\n",
            "----------\n",
            "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
            "train Loss: 0.0026 Acc: 0.7737\n",
            "val Loss: 0.0030 Acc: 0.7434\n",
            "\n",
            "Epoch 64/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7750\n",
            "val Loss: 0.0029 Acc: 0.7490\n",
            "\n",
            "Epoch 65/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7775\n",
            "val Loss: 0.0030 Acc: 0.7446\n",
            "\n",
            "Epoch 66/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7743\n",
            "val Loss: 0.0029 Acc: 0.7468\n",
            "\n",
            "Epoch 67/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7782\n",
            "val Loss: 0.0029 Acc: 0.7478\n",
            "\n",
            "Epoch 68/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7787\n",
            "val Loss: 0.0029 Acc: 0.7540\n",
            "\n",
            "Epoch 69/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7773\n",
            "val Loss: 0.0030 Acc: 0.7476\n",
            "\n",
            "Epoch 70/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7781\n",
            "val Loss: 0.0029 Acc: 0.7514\n",
            "\n",
            "Epoch 71/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7818\n",
            "val Loss: 0.0029 Acc: 0.7474\n",
            "\n",
            "Epoch 72/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7829\n",
            "val Loss: 0.0030 Acc: 0.7412\n",
            "\n",
            "Epoch 73/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7784\n",
            "val Loss: 0.0029 Acc: 0.7538\n",
            "\n",
            "Epoch 74/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7801\n",
            "val Loss: 0.0029 Acc: 0.7530\n",
            "\n",
            "Epoch 75/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7820\n",
            "val Loss: 0.0029 Acc: 0.7504\n",
            "\n",
            "Epoch 76/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7814\n",
            "val Loss: 0.0029 Acc: 0.7498\n",
            "\n",
            "Epoch 77/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7828\n",
            "val Loss: 0.0029 Acc: 0.7598\n",
            "\n",
            "Epoch 78/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7797\n",
            "val Loss: 0.0029 Acc: 0.7542\n",
            "\n",
            "Epoch 79/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7829\n",
            "val Loss: 0.0029 Acc: 0.7490\n",
            "\n",
            "Epoch 80/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7826\n",
            "val Loss: 0.0029 Acc: 0.7610\n",
            "\n",
            "Epoch 81/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7822\n",
            "val Loss: 0.0028 Acc: 0.7572\n",
            "\n",
            "Epoch 82/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7826\n",
            "val Loss: 0.0029 Acc: 0.7510\n",
            "\n",
            "Epoch 83/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7811\n",
            "val Loss: 0.0029 Acc: 0.7468\n",
            "\n",
            "Epoch 84/199\n",
            "----------\n",
            "Epoch 00085: reducing learning rate of group 0 to 1.0000e-05.\n",
            "train Loss: 0.0025 Acc: 0.7833\n",
            "val Loss: 0.0030 Acc: 0.7456\n",
            "\n",
            "Epoch 85/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7850\n",
            "val Loss: 0.0029 Acc: 0.7548\n",
            "\n",
            "Epoch 86/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7849\n",
            "val Loss: 0.0029 Acc: 0.7488\n",
            "\n",
            "Epoch 87/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7838\n",
            "val Loss: 0.0029 Acc: 0.7586\n",
            "\n",
            "Epoch 88/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7821\n",
            "val Loss: 0.0029 Acc: 0.7560\n",
            "\n",
            "Epoch 89/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7835\n",
            "val Loss: 0.0029 Acc: 0.7536\n",
            "\n",
            "Epoch 90/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7848\n",
            "val Loss: 0.0030 Acc: 0.7498\n",
            "\n",
            "Epoch 91/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7830\n",
            "val Loss: 0.0030 Acc: 0.7446\n",
            "\n",
            "Epoch 92/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7822\n",
            "val Loss: 0.0030 Acc: 0.7492\n",
            "\n",
            "Epoch 93/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7819\n",
            "val Loss: 0.0028 Acc: 0.7536\n",
            "\n",
            "Epoch 94/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7846\n",
            "val Loss: 0.0029 Acc: 0.7538\n",
            "\n",
            "Epoch 95/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7832\n",
            "val Loss: 0.0029 Acc: 0.7488\n",
            "\n",
            "Epoch 96/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7857\n",
            "val Loss: 0.0029 Acc: 0.7480\n",
            "\n",
            "Epoch 97/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7823\n",
            "val Loss: 0.0029 Acc: 0.7532\n",
            "\n",
            "Epoch 98/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7806\n",
            "val Loss: 0.0028 Acc: 0.7546\n",
            "\n",
            "Epoch 99/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7829\n",
            "val Loss: 0.0029 Acc: 0.7540\n",
            "\n",
            "Epoch 100/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7833\n",
            "val Loss: 0.0029 Acc: 0.7494\n",
            "\n",
            "Epoch 101/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7822\n",
            "val Loss: 0.0029 Acc: 0.7512\n",
            "\n",
            "Epoch 102/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7839\n",
            "val Loss: 0.0029 Acc: 0.7544\n",
            "\n",
            "Epoch 103/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7847\n",
            "val Loss: 0.0029 Acc: 0.7548\n",
            "\n",
            "Epoch 104/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7861\n",
            "val Loss: 0.0029 Acc: 0.7542\n",
            "\n",
            "Epoch 105/199\n",
            "----------\n",
            "Epoch 00106: reducing learning rate of group 0 to 1.0000e-06.\n",
            "train Loss: 0.0024 Acc: 0.7829\n",
            "val Loss: 0.0029 Acc: 0.7504\n",
            "\n",
            "Epoch 106/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7822\n",
            "val Loss: 0.0029 Acc: 0.7580\n",
            "\n",
            "Epoch 107/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7853\n",
            "val Loss: 0.0029 Acc: 0.7502\n",
            "\n",
            "Epoch 108/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7825\n",
            "val Loss: 0.0029 Acc: 0.7508\n",
            "\n",
            "Epoch 109/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7823\n",
            "val Loss: 0.0029 Acc: 0.7560\n",
            "\n",
            "Epoch 110/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7882\n",
            "val Loss: 0.0030 Acc: 0.7436\n",
            "\n",
            "Epoch 111/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7841\n",
            "val Loss: 0.0029 Acc: 0.7582\n",
            "\n",
            "Epoch 112/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7844\n",
            "val Loss: 0.0029 Acc: 0.7572\n",
            "\n",
            "Epoch 113/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7834\n",
            "val Loss: 0.0029 Acc: 0.7502\n",
            "\n",
            "Epoch 114/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7844\n",
            "val Loss: 0.0029 Acc: 0.7504\n",
            "\n",
            "Epoch 115/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7844\n",
            "val Loss: 0.0029 Acc: 0.7534\n",
            "\n",
            "Epoch 116/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7841\n",
            "val Loss: 0.0029 Acc: 0.7572\n",
            "\n",
            "Epoch 117/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7838\n",
            "val Loss: 0.0029 Acc: 0.7604\n",
            "\n",
            "Epoch 118/199\n",
            "----------\n",
            "train Loss: 0.0024 Acc: 0.7859\n",
            "val Loss: 0.0029 Acc: 0.7544\n",
            "\n",
            "Epoch 119/199\n",
            "----------\n",
            "train Loss: 0.0025 Acc: 0.7835\n",
            "val Loss: 0.0029 Acc: 0.7494\n",
            "\n",
            "Epoch 120/199\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6gXMLcaY7Wi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHPXq2DWEN-J",
        "outputId": "d0dc821a-7408-4eab-e6f8-0301368d9187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.datasets import cifar\n",
        "\n",
        "import copy\n",
        "import time\n",
        "\n",
        "\n",
        "sample_data = cifar.CIFAR10('./', download=True)\n",
        "\n",
        "means = sample_data.data.mean(axis=(0, 1, 2)) / 255\n",
        "stds = sample_data.data.std(axis=(0, 1, 2)) / 255\n",
        "\n",
        "img_transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                     transforms.Normalize(means, stds)])\n",
        "\n",
        "train_data = cifar.CIFAR10('./', download=True, transform=img_transforms)\n",
        "test_data = cifar.CIFAR10('./', train=False, download=True,\n",
        "                             transform=img_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_RATIO = 0.9\n",
        "num_train_samples = int(len(train_data) * TRAIN_RATIO)\n",
        "num_valid_samples = len(train_data) - num_train_samples\n",
        "split = [num_train_samples, num_valid_samples]\n",
        "\n",
        "train_data, valid_data = data.random_split(train_data, lengths=split)"
      ],
      "metadata": {
        "id": "i6WF0sLHEpGr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transforms = img_transforms"
      ],
      "metadata": {
        "id": "27LJJCmnFA8b"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_iterator = data.DataLoader(train_data, BATCH_SIZE, shuffle=True)\n",
        "valid_iterator = data.DataLoader(valid_data, BATCH_SIZE)\n",
        "test_iterator = data.DataLoader(test_data, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "z7y5OgnIWGl7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloaders = {\"train\": train_iterator, \"val\": valid_iterator}\n",
        "dataset_sizes = {\"train\": len(train_data.indices),\n",
        "                 'val': len(valid_data.indices)}"
      ],
      "metadata": {
        "id": "TAfhGR-mYGsc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':  # take scheduler step on train acc\n",
        "                scheduler.step(epoch_acc)\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "yH5ZK5_pQeZS"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 200\n",
        "LR = 0.1\n",
        "LR_DECAY = 0.1\n",
        "\n",
        "model = resnet18()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 10)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# same parameters as the ResNet paper\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.99,\n",
        "                            weight_decay=1e-4)\n",
        "\n",
        "# patience is not known from the paper\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
        "                                                       factor=LR_DECAY,\n",
        "                                                       patience=NUM_EPOCHS / 10,\n",
        "                                                       verbose=True)\n",
        "\n",
        "model.to(DEVICE)\n",
        "criterion.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZh2hSjOL_8I",
        "outputId": "826fc60a-9019-4bec-bad1-569f7b1da177"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, criterion, optimizer, scheduler, num_epochs=NUM_EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N0cUIZOKXRro",
        "outputId": "c57e635b-6325-4552-9b3b-211dc006300e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/199\n",
            "----------\n",
            "train Loss: 0.0102 Acc: 0.2106\n",
            "val Loss: 0.0099 Acc: 0.2678\n",
            "\n",
            "Epoch 1/199\n",
            "----------\n",
            "train Loss: 0.0073 Acc: 0.2937\n",
            "val Loss: 0.0069 Acc: 0.3462\n",
            "\n",
            "Epoch 2/199\n",
            "----------\n",
            "train Loss: 0.0067 Acc: 0.3347\n",
            "val Loss: 0.0068 Acc: 0.3586\n",
            "\n",
            "Epoch 3/199\n",
            "----------\n",
            "train Loss: 0.0064 Acc: 0.3918\n",
            "val Loss: 0.0064 Acc: 0.3996\n",
            "\n",
            "Epoch 4/199\n",
            "----------\n",
            "train Loss: 0.0060 Acc: 0.4360\n",
            "val Loss: 0.0061 Acc: 0.4278\n",
            "\n",
            "Epoch 5/199\n",
            "----------\n",
            "train Loss: 0.0057 Acc: 0.4774\n",
            "val Loss: 0.0060 Acc: 0.4438\n",
            "\n",
            "Epoch 6/199\n",
            "----------\n",
            "train Loss: 0.0055 Acc: 0.5012\n",
            "val Loss: 0.0055 Acc: 0.5124\n",
            "\n",
            "Epoch 7/199\n",
            "----------\n",
            "train Loss: 0.0054 Acc: 0.5159\n",
            "val Loss: 0.0055 Acc: 0.5152\n",
            "\n",
            "Epoch 8/199\n",
            "----------\n",
            "train Loss: 0.0053 Acc: 0.5293\n",
            "val Loss: 0.0052 Acc: 0.5506\n",
            "\n",
            "Epoch 9/199\n",
            "----------\n",
            "train Loss: 0.0047 Acc: 0.5862\n",
            "val Loss: 0.0046 Acc: 0.6058\n",
            "\n",
            "Epoch 10/199\n",
            "----------\n",
            "train Loss: 0.0042 Acc: 0.6321\n",
            "val Loss: 0.0041 Acc: 0.6420\n",
            "\n",
            "Epoch 11/199\n",
            "----------\n",
            "train Loss: 0.0039 Acc: 0.6555\n",
            "val Loss: 0.0047 Acc: 0.6328\n",
            "\n",
            "Epoch 12/199\n",
            "----------\n",
            "train Loss: 0.0037 Acc: 0.6807\n",
            "val Loss: 0.0041 Acc: 0.6594\n",
            "\n",
            "Epoch 13/199\n",
            "----------\n",
            "train Loss: 0.0035 Acc: 0.6951\n",
            "val Loss: 0.0041 Acc: 0.6584\n",
            "\n",
            "Epoch 14/199\n",
            "----------\n",
            "train Loss: 0.0034 Acc: 0.7045\n",
            "val Loss: 0.0041 Acc: 0.6590\n",
            "\n",
            "Epoch 15/199\n",
            "----------\n",
            "train Loss: 0.0034 Acc: 0.7095\n",
            "val Loss: 0.0044 Acc: 0.6506\n",
            "\n",
            "Epoch 16/199\n",
            "----------\n",
            "train Loss: 0.0034 Acc: 0.7103\n",
            "val Loss: 0.0038 Acc: 0.6890\n",
            "\n",
            "Epoch 17/199\n",
            "----------\n",
            "train Loss: 0.0032 Acc: 0.7280\n",
            "val Loss: 0.0038 Acc: 0.6810\n",
            "\n",
            "Epoch 18/199\n",
            "----------\n",
            "train Loss: 0.0030 Acc: 0.7470\n",
            "val Loss: 0.0037 Acc: 0.6890\n",
            "\n",
            "Epoch 19/199\n",
            "----------\n",
            "train Loss: 0.0029 Acc: 0.7494\n",
            "val Loss: 0.0039 Acc: 0.6960\n",
            "\n",
            "Epoch 20/199\n",
            "----------\n",
            "train Loss: 0.0027 Acc: 0.7702\n",
            "val Loss: 0.0037 Acc: 0.6848\n",
            "\n",
            "Epoch 21/199\n",
            "----------\n",
            "Epoch 00022: reducing learning rate of group 0 to 1.0000e-02.\n",
            "train Loss: 0.0026 Acc: 0.7788\n",
            "val Loss: 0.0037 Acc: 0.7028\n",
            "\n",
            "Epoch 22/199\n",
            "----------\n",
            "train Loss: 0.0018 Acc: 0.8442\n",
            "val Loss: 0.0032 Acc: 0.7480\n",
            "\n",
            "Epoch 23/199\n",
            "----------\n",
            "train Loss: 0.0013 Acc: 0.8894\n",
            "val Loss: 0.0032 Acc: 0.7568\n",
            "\n",
            "Epoch 24/199\n",
            "----------\n",
            "train Loss: 0.0010 Acc: 0.9174\n",
            "val Loss: 0.0034 Acc: 0.7560\n",
            "\n",
            "Epoch 25/199\n",
            "----------\n",
            "train Loss: 0.0008 Acc: 0.9366\n",
            "val Loss: 0.0038 Acc: 0.7572\n",
            "\n",
            "Epoch 26/199\n",
            "----------\n",
            "train Loss: 0.0006 Acc: 0.9538\n",
            "val Loss: 0.0042 Acc: 0.7576\n",
            "\n",
            "Epoch 27/199\n",
            "----------\n",
            "train Loss: 0.0004 Acc: 0.9684\n",
            "val Loss: 0.0048 Acc: 0.7584\n",
            "\n",
            "Epoch 28/199\n",
            "----------\n",
            "train Loss: 0.0003 Acc: 0.9756\n",
            "val Loss: 0.0052 Acc: 0.7580\n",
            "\n",
            "Epoch 29/199\n",
            "----------\n",
            "train Loss: 0.0003 Acc: 0.9793\n",
            "val Loss: 0.0055 Acc: 0.7510\n",
            "\n",
            "Epoch 30/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9800\n",
            "val Loss: 0.0057 Acc: 0.7454\n",
            "\n",
            "Epoch 31/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9811\n",
            "val Loss: 0.0058 Acc: 0.7494\n",
            "\n",
            "Epoch 32/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9801\n",
            "val Loss: 0.0058 Acc: 0.7478\n",
            "\n",
            "Epoch 33/199\n",
            "----------\n",
            "train Loss: 0.0003 Acc: 0.9778\n",
            "val Loss: 0.0057 Acc: 0.7472\n",
            "\n",
            "Epoch 34/199\n",
            "----------\n",
            "train Loss: 0.0003 Acc: 0.9793\n",
            "val Loss: 0.0056 Acc: 0.7484\n",
            "\n",
            "Epoch 35/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9814\n",
            "val Loss: 0.0057 Acc: 0.7404\n",
            "\n",
            "Epoch 36/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9844\n",
            "val Loss: 0.0057 Acc: 0.7480\n",
            "\n",
            "Epoch 37/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9855\n",
            "val Loss: 0.0057 Acc: 0.7528\n",
            "\n",
            "Epoch 38/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9866\n",
            "val Loss: 0.0058 Acc: 0.7536\n",
            "\n",
            "Epoch 39/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9878\n",
            "val Loss: 0.0057 Acc: 0.7514\n",
            "\n",
            "Epoch 40/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9864\n",
            "val Loss: 0.0059 Acc: 0.7492\n",
            "\n",
            "Epoch 41/199\n",
            "----------\n",
            "train Loss: 0.0002 Acc: 0.9842\n",
            "val Loss: 0.0057 Acc: 0.7460\n",
            "\n",
            "Epoch 42/199\n",
            "----------\n",
            "Epoch 00043: reducing learning rate of group 0 to 1.0000e-03.\n",
            "train Loss: 0.0002 Acc: 0.9844\n",
            "val Loss: 0.0057 Acc: 0.7482\n",
            "\n",
            "Epoch 43/199\n",
            "----------\n",
            "train Loss: 0.0001 Acc: 0.9925\n",
            "val Loss: 0.0055 Acc: 0.7592\n",
            "\n",
            "Epoch 44/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9976\n",
            "val Loss: 0.0055 Acc: 0.7614\n",
            "\n",
            "Epoch 45/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9983\n",
            "val Loss: 0.0056 Acc: 0.7608\n",
            "\n",
            "Epoch 46/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9986\n",
            "val Loss: 0.0056 Acc: 0.7600\n",
            "\n",
            "Epoch 47/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9988\n",
            "val Loss: 0.0056 Acc: 0.7592\n",
            "\n",
            "Epoch 48/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9988\n",
            "val Loss: 0.0056 Acc: 0.7612\n",
            "\n",
            "Epoch 49/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9988\n",
            "val Loss: 0.0057 Acc: 0.7602\n",
            "\n",
            "Epoch 50/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9990\n",
            "val Loss: 0.0057 Acc: 0.7616\n",
            "\n",
            "Epoch 51/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9990\n",
            "val Loss: 0.0057 Acc: 0.7612\n",
            "\n",
            "Epoch 52/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9990\n",
            "val Loss: 0.0057 Acc: 0.7610\n",
            "\n",
            "Epoch 53/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9990\n",
            "val Loss: 0.0058 Acc: 0.7618\n",
            "\n",
            "Epoch 54/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9990\n",
            "val Loss: 0.0057 Acc: 0.7614\n",
            "\n",
            "Epoch 55/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9991\n",
            "val Loss: 0.0058 Acc: 0.7612\n",
            "\n",
            "Epoch 56/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9992\n",
            "val Loss: 0.0058 Acc: 0.7624\n",
            "\n",
            "Epoch 57/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9992\n",
            "val Loss: 0.0059 Acc: 0.7624\n",
            "\n",
            "Epoch 58/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9991\n",
            "val Loss: 0.0059 Acc: 0.7618\n",
            "\n",
            "Epoch 59/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9993\n",
            "val Loss: 0.0059 Acc: 0.7608\n",
            "\n",
            "Epoch 60/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9992\n",
            "val Loss: 0.0059 Acc: 0.7624\n",
            "\n",
            "Epoch 61/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9992\n",
            "val Loss: 0.0059 Acc: 0.7612\n",
            "\n",
            "Epoch 62/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9993\n",
            "val Loss: 0.0059 Acc: 0.7624\n",
            "\n",
            "Epoch 63/199\n",
            "----------\n",
            "Epoch 00064: reducing learning rate of group 0 to 1.0000e-04.\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7610\n",
            "\n",
            "Epoch 64/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7606\n",
            "\n",
            "Epoch 65/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7602\n",
            "\n",
            "Epoch 66/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9993\n",
            "val Loss: 0.0059 Acc: 0.7614\n",
            "\n",
            "Epoch 67/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9995\n",
            "val Loss: 0.0059 Acc: 0.7608\n",
            "\n",
            "Epoch 68/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9993\n",
            "val Loss: 0.0059 Acc: 0.7620\n",
            "\n",
            "Epoch 69/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9993\n",
            "val Loss: 0.0059 Acc: 0.7608\n",
            "\n",
            "Epoch 70/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7616\n",
            "\n",
            "Epoch 71/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7614\n",
            "\n",
            "Epoch 72/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7588\n",
            "\n",
            "Epoch 73/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7594\n",
            "\n",
            "Epoch 74/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9993\n",
            "val Loss: 0.0060 Acc: 0.7612\n",
            "\n",
            "Epoch 75/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7614\n",
            "\n",
            "Epoch 76/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7592\n",
            "\n",
            "Epoch 77/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7588\n",
            "\n",
            "Epoch 78/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9995\n",
            "val Loss: 0.0059 Acc: 0.7618\n",
            "\n",
            "Epoch 79/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7600\n",
            "\n",
            "Epoch 80/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9995\n",
            "val Loss: 0.0059 Acc: 0.7600\n",
            "\n",
            "Epoch 81/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7600\n",
            "\n",
            "Epoch 82/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7598\n",
            "\n",
            "Epoch 83/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7602\n",
            "\n",
            "Epoch 84/199\n",
            "----------\n",
            "Epoch 00085: reducing learning rate of group 0 to 1.0000e-05.\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7602\n",
            "\n",
            "Epoch 85/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7604\n",
            "\n",
            "Epoch 86/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7602\n",
            "\n",
            "Epoch 87/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0060 Acc: 0.7606\n",
            "\n",
            "Epoch 88/199\n",
            "----------\n",
            "train Loss: 0.0000 Acc: 0.9994\n",
            "val Loss: 0.0059 Acc: 0.7596\n",
            "\n",
            "Epoch 89/199\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f3ee29243d8f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-bf1685769179>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# Iterate over data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be Tensor Image. Got {type(tensor)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/transforms/_functional_tensor.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    920\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"std evaluated to zero after conversion to {dtype}, leading to division by zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6gXMLcaY7Wi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}